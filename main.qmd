---
pagetitle: "Bayesian estimation for ecology"
author: "Masatoshi Katabuchi"
date: "November 10, 2024"
host: "XTBG AFEC"
institute: "XTBG, CAS"
strip-comments: true
format:
  revealjs:
    html-math-method: mathjax
    logo: images/xtbg_logo.png
    chalkboard: true
    slide-number: true
    multiplex: true
    theme: assets/rladies.scss
    show-slide-number: all
    controls: true
    width: 1440
    height: 810
    css: [assets/custom.css]
    include-after: |
      <link rel="stylesheet" href="assets/syntax-highlight.css">
      <link rel="stylesheet" href="assets/fontawesome-free-6.1.1-web/css/all.min.css">
      <script src="assets/fontawesome-free-6.1.1-web/js/all.min.js"></script>
callout-icon: false
execute:
  echo: true
---

```{r external, child="setup.Rmd", include=FALSE}
```


##  {#title-slide background="images/priors.png" background-size="50%" background-position="50% 20%"}

::: title-box
<h2>

`r rmarkdown::metadata$pagetitle`

</h2>

<h3>
üßëüèª‚Äçüíª [`r rmarkdown::metadata$author` \@ `r rmarkdown::metadata$institute`]{.author}


`r rotating_text(c('<i class="fa-solid fa-envelope"></i> mattocci27@gmail.com', '<i class="fa-brands fa-twitter"></i> @mattocci', '<i class="fa-brands fa-github"></i> github.com/mattocci27/bayes-afec', '<i class="fa-solid fa-globe"></i> https://mattocci27.github.io'))`
</h3>
:::


<br><br>

::: {.absolute .top-0 .w-100}
[`r rmarkdown::metadata$date`]{.fl} [`r rmarkdown::metadata$host`]{.fr}
:::


::: notes
hoge
:::

---

## Objectives

::: incremental
::: large
- We Learn

  - Why Bayesian estimation is useful

  - Why multilevel models are important

- We Do Not Learn

  - How to use and code Bayesian models in Stan and R
:::
:::

::: notes
- coding exercises are not the focus of this workshop
- it will take a couple of days
- idea of Bayesian in ecology
:::


# Likelihood

::: fragment
::: large
Assuming everyone knows the concept of likelihood
:::
:::

::: notes
correct?
:::

---

## Likelihood and probability density distribution

::: columns

::: {.column width=40%}

```{r, fig.height=7}
x <- seq(-5, 5, length.out = 100)
y <- dnorm(x, mean = 0, sd = 1)
tibble(x, y) |>
  ggplot() +
  xlab("x") +
  ylab("Density") +
  ggtitle("N(0, 1)") +
  geom_line(aes(x, y)) +
  theme(
    axis.title   = element_text(size = 32),
    axis.text    = element_text(size = 28))
```

::: small
$f(x \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp} \bigl[{-\frac{1}{2}\bigl\{\frac{x-\mu}{\sigma}}\bigr\}^2\bigr]$
:::

:::
::: {.column width=60%}
::: incremental
- A likelihood function is a probability function that shows how likely it is to observe the data for a specific set of parameters.

- e.g., when your model is $x \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)$ (normal distribution with mean 0 and variance 1), what is the probability density at x = 1.96?
:::

::: fragment
```{r, echo=TRUE}
1 / sqrt(2 * pi * 1) * exp(-1/2 * (1.96 - 0)^2 / 1)
```
:::

::: fragment
- $P(x = 1.96 \mid \mu = 0, \sigma = 1)$
:::

::: fragment
```{r, echo=TRUE}
dnorm(1.96, mean = 0, sd = 1)
```
:::

:::

:::

::: notes
- likelihood is all about probability density (mass) function (normal, log-normal, poisson, beta, gamma, binomial, negative-binomial...)
:::

---

## Likelihood

When your data is x = {-1.5, 0, 1.5} and your model is $x \sim \mathcal{N}(0, 1)$, what is the probability of observing x?

::: small
::: incremental
- $L = P(-1.5 \mid 0, 1) \times P(0 \mid 0, 1) \times P(1.5 \mid 0, 1)$
- $\mathrm{ln}\;L = \mathrm{ln}\;P(-1.5 \mid 0, 1) + \mathrm{ln}\;P(0 \mid 0, 1) +  \mathrm{ln}\;P(1.5 \mid 0, 1)$
:::
:::

::: fragment
```{r, echo=TRUE}
dnorm(-1.5, mean = 0, sd = 1, log = TRUE) + dnorm(0, mean = 0, sd = 1, log = TRUE) + dnorm(1.5, mean = 0, sd = 1, log = TRUE)
```
:::

::: fragment

```{r, fig.height=3.5}
like_fun <- function(mu) {
  dnorm(-1.5, mean = mu, sd = 1, log = TRUE) + dnorm(0, mean = mu, sd = 1, log = TRUE) + dnorm(1.5, mean = mu, sd = 1, log = TRUE)
}

x <- seq(-5, 5, length.out = 100)
y <- like_fun(x)
tibble(x, y) |>
  ggplot() +
  xlab("mu") +
  ylab("ln L") +
  ggtitle("N(0, 1)") +
  geom_line(aes(x, y)) +
  theme(
    axis.title   = element_text(size = 32),
    axis.text    = element_text(size = 28))

```
:::


## Likelihood

- Because we usually don't know the true parameters ($\mu$ and $\sigma$), we need to find the parameters that maximize the likelihood function (**Maximum Likelihood Estimation**).

  - e.g., for a liner model $y = ax + b$, we usually assume that $y \sim \mathcal{N}(\mu = ax + b, \sigma^2)$, and we want to find the parameters $a$, $b$, and $\sigma$ that maximize the likelihood function: $P(y \mid ax + b, \sigma)$

---

## Maximum Likelihood Estimation (MLE)

- 2 survivors out of 5 seedlings: What is the survival probability of seedlings?

::: columns

::: {.column width=40%}
![](images/seedling.png)
:::

::: {.column width=60%}
::: incremental
- $p$: survival rates, $1-p$: mortality rate

- $L = {}_5C_2 p^2(1-p)^3$ (Binomial distribution)

- $\mathrm{ln}\;L = \mathrm{ln}\;{}_5C_2 + 2\mathrm{ln}\;p + 3\mathrm{ln}(1-p)$

- $\frac{d\mathrm{ln}\;L}{dt} = \frac{2}{p} - \frac{3}{1-p}  = 0$

- $p = \frac{2}{5}$
:::
:::
:::

---

## Outline

- Multilevel Model

- Conditional Probability and Bayes's Theorem

- Prior

- Multilevel Model Revisited

# Multilevel Models

---

::: columns
::: {.column width="40%"}
![](images/comita_cover.jpg)
:::

::: {.column width="60%"}
### Negative density dependence (NDD)

"...rare species suffered more from the presence of conspecific neighbors than common species did, suggesting that conspecific density dependence shapes species abundances in diverse communities."

\-                    Comita et al. 2010 \-
:::
:::

::: footer
[1] Comita, L. S., Muller-Landau, H. C., Aguilar, S. & Hubbell, S. P. Asymmetric density dependence shapes species abundances in a tropical tree community. Science 329, 330‚Äì2 (2010).
:::

---


## Multilevel model (NDD: verbal model)

::: footer
Inspired by [1] Comita, L. S., Muller-Landau, H. C., Aguilar, S. & Hubbell, S. P. Asymmetric density dependence shapes species abundances in a tropical tree community. Science 329, 330‚Äì2 (2010).
:::

::: small
There is negative density dependence (NDD) of seedling survival rate, and the strength of NDD varies among species.
The strength of NDD depends on species abundance.

- Model survival rates as a function of conspecific seedling density (individual-level).
- Model the strength of NDD (i.e., slopes) as a function of species abundance (group-level).
:::

```{r, echo=FALSE, fig.width=15, fig.height=5, fig.retina=3}
set.seed(123)
n_sp <- 10
n_rep <- 80
trait <- rnorm(n_sp, 0, 1)
gamma0 <- -0.8
gamma1 <- 0.3
b1_hat <- gamma1 * trait + gamma0
b1 <- rnorm(n_sp, b1_hat, 0.1)

b0 <- rnorm(n_sp, 0, 0.5)
y_fun <- function(beta0, beta1) beta0 + beta1 * xx
xx <- seq(-3, 3, length = n_rep)
z <- map2(as.list(b0), as.list(b1), y_fun) |> unlist()

tmp <- tibble(z,
              x = rep(xx, n_sp),
              sp = rep(paste0("sp", 1:n_sp), each = n_rep))

tmp2 <- tibble(x = trait, y = b1, sp = rep(paste0("sp", 1:n_sp)))
logistic <- function(z) 1 / (1 + exp(-z))

p1 <- tmp |>
  ggplot(aes(x = x, y = logistic(z), col = sp)) +
  geom_line() +
  xlab("Conspecific density") +
  ylab("Survival rates") +
  theme(legend.position = "none")

p2 <- tmp |>
  ggplot(aes(x = x, y = z, col = sp)) +
  geom_line() +
  xlab("Conspecific density") +
  ylab("logit(Survival rates)") +
  theme(legend.position = "none")

p3 <- tmp2 |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_smooth(method = "lm", se = TRUE, lty = 2,
    alpha = 0.2,
    linewidth = 0.5, col = "black") +
  geom_point(size = 3) +
  xlab("Abundance") +
  ylab("Strength of NDD") +
  theme(legend.position = "none")

p1 + p2 + p3
```
## Multilevel model (tree allometry: verbal model)

::: footer
Inspired by [1]
Mart√≠nez Cano *et al.* Tropical Tree Height and Crown Allometries for the Barro Colorado Nature Monument, Panama: A Comparison of Alternative Hierarchical Models Incorporating Interspecific Variation in Relation to Life History Traits. Biogeosciences 16, 847‚Äì62 (2019).
:::

::: small
There is a power-law relationship ($y = ax^b$) between tree diameter (DBH) and crown area, and the power-law exponent varies among species.
Those relationships depend on wood density.

- Model crown area as a function of DBH (individual-level).

- Model the coefficient *b* as a function of wood density (group-level).
:::

```{r, echo=FALSE, fig.width=15, fig.height=5, fig.retina=3}
set.seed(12345)

n_sp <- 10
n_rep <- 80
trait <- rnorm(n_sp, 0, 1)
gamma0 <- 1.35
gamma1 <- 0.05
b1_hat <- gamma1 * trait + gamma0
b1 <- rnorm(n_sp, b1_hat, 0.01)
b0 <- rnorm(n_sp, 0.55, 0.05)
y_fun <- function(beta0, beta1) beta0 + beta1 * log_xx
log_xx <- seq(1, 100, length = n_rep) |> log()

log_y <- map2(as.list(b0), as.list(b1), y_fun) |> unlist()

tmp <- tibble(y = exp(log_y),
              x = rep(exp(log_xx), n_sp),
              sp = rep(paste0("sp", 1:n_sp), each = n_rep))

tmp2 <- tibble(x = trait, y = b1, sp = rep(paste0("sp", 1:n_sp)))

p1 <- tmp |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_line() +
  xlab("DBH (cm)") +
  ylab(expression("Crown area (m"^2*")")) +
  ggtitle(expression("y = ax"^b)) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24)
    )


p2 <- tmp |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_line() +
  xlab("DBH (cm)") +
  ylab(expression("Crown area (m"^2*")")) +
  ggtitle("log(y) = log(a) + b log(x)") +
  scale_x_log10() +
  scale_y_log10() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24)
    )

p3 <- tmp2 |>
  ggplot(aes(x = x, y = y, col = sp)) +
  geom_smooth(method = "lm", se = TRUE, lty = 2,
    alpha = 0.2,
    linewidth = 0.5, col = "black") +
  geom_point(size = 3) +
  xlab("Wood density") +
  ylab("b") +
  theme(legend.position = "none")

p1 + p2 + p3
```

---

## Multilevel model (MLE: non-verbal 'dummy' model 1)

```{r, echo=FALSE, eval=FALSE}
library(tidyverse)
library(targets)
tar_load(dummy_simple)
```

### Individual-level model: GLMM

```{r, echo=TRUE, eval=FALSE}
fit_ind <- lme4::glmer(
  cbind(suv, n - suv) ~  cons + (1 + cons | sp),
  data = dummy, family = binomial)
```

We need to extract the slope coefficients for each species from `fit_ind` and make `new_data`.

::: fragment
### Group-level model: LM

```{r, echo=TRUE, eval=FALSE}
fit_gr <- lm(slope_coef ~  abund, data = new_data)
```
:::

::: incremental
- This method have low statistical power
:::

---

## Multilevel model (MLE: non-verbal 'dummy' model 2)

```{r, echo=TRUE, eval=FALSE}
fit_all <- lme4::glmer(
  cbind(suv, n - suv) ~  cons + abund + cons:abund + (1 + cons | sp),
  data = dummy, family = binomial)
```

::: incremental
- This doesn't work for complicated models
:::

---

## Multilevel model (Bayes: non-verbal model) `r emo::ji("vomit")`

::: footer
Inspired by [1] Comita, L. S., Muller-Landau, H. C., Aguilar, S. & Hubbell, S. P. Asymmetric density dependence shapes species abundances in a tropical tree community. Science 329, 330‚Äì2 (2010).
:::

::: columns

::: {.column width=50%}
```{stan, output.var="mv_logistic",file=here('stan/mv_logistic.stan'), eval=FALSE, echo=TRUE}
```
:::

::: {.column width=50%}
::: small
- $s_{i,j} \sim \mathcal{B}(p_{i, j})$: [likelihood]{.orange}

- $\mathrm{logit}(p_{i,j}) = \boldsymbol{x_{i}} \cdot \boldsymbol{\beta_{j}}$: individual-level model

- $\boldsymbol{\beta_j} = \boldsymbol{\gamma_k} \cdot \boldsymbol{u_j} + \mathrm{diag}(\boldsymbol{\sigma})\cdot \boldsymbol{L} \cdot \boldsymbol{z}$: species-level model

- $L \sim \mathrm{LkjCholesky}(\eta)$: [prior (?)]{.blue}

- $z_j \sim \mathcal{N}(0, 1)$: [prior (?)]{.blue}

- $\tau \sim \mathrm{Cauchy}(0, 2.5)$: [prior (?)]{.blue}

- $\gamma_k \sim \mathcal{N}(0, 2.5)$: [prior (?)]{.blue}

:::
:::

:::

# Conditional Probability and Bayes's Theorem

## Conditional Probability and Bayes's Theorem

$$
P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}
$$

- Conditional probability

- Bayes's Theorem

- Forward / inverse problems

- Bayes revisit

---

## Probability

::: columns
::: {.column width="50%"}

```{r}
ggplot() +
  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1.5) +
  ggtitle("U") +
  theme_void() +
  annotate("text", x = 0, y = 0, label = "A", size = 10) +
  theme(
      title = element_text(size = 30),
      panel.border = element_rect(colour = "black", size = 2),
      legend.position = "none"
    )
```
:::
::: {.column width="50%"}

Probility of A:

$$
P(A) = \frac{A}{U}
$$

e.g., probability of rolling a dice and getting an odd number is 3/6 = 1/2

:::
:::


---

## Conditional Probability

::: columns

::: {.column width="40%"}

```{r, eval=FALSE}
ggplot() +
  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1.5) +
  ggtitle("U") +
  theme_void() +
  theme(
    panel.border = element_rect(colour = "black")
  )
```

```{r}
a <- 1
df.venn <- data.frame(x = c(-a, a),
                      y = c(0, 0),
                      labels = c("A", "B"),
                      stringsAsFactors = FALSE)

radius <- 1.5

xvals <- seq(0, 2 * a - radius, 0.01)
yvals <- sqrt(radius^2 - (xvals + a)^2)
xvals <- c(xvals, rev(xvals))
yvals <- c(yvals, -rev(yvals))
xvals <- c(xvals, -xvals)
yvals <- c(yvals, -yvals)
combo <- data.frame(x = xvals, y = yvals)

xvals <- seq(a - radius, a  + radius, 0.01)
yvals <- sqrt(radius^2 - (xvals - a)^2)
xvals <- c(xvals, rev(xvals))
yvals <- c(yvals, -yvals)
combo2 <- data.frame(x = xvals, y = yvals)

ggplot2::ggplot(data = df.venn) +
    ggforce::geom_circle(
        ggplot2::aes_string(x0 = "x", y0 = "y", r = radius, fill = "labels"),
        alpha = 0,
        size = 1.5
#       colour = 'darkgray'
    ) +
    ggtitle("U") +
    ggplot2::geom_polygon(data = combo, aes(x = x, y = y),
      fill = "blue", alpha = 0.6) +
    ggplot2::geom_polygon(data = combo2, aes(x = x, y = y),
      fill = "blue", alpha = 0.3) +
    ggplot2::coord_fixed() +
    ggplot2::theme_void() +
    ggplot2::scale_fill_manual(values = c("gray50", "gray50")) +
    annotate("text", x = 0, y = 0, label = "A ‚à© B", size = 10) +
    annotate("text", x = -1, y = 0, label = "A", size = 10) +
    annotate("text", x = 1, y = 0, label = "B", size = 10) +
    theme(
      title = element_text(size = 30),
      panel.border = element_rect(colour = "black", size = 2),
      legend.position = "none"
    )

```

:::

::: {.column width="60%"}

Probability of A occurring given B has already occurred:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$

::: incremental
e.g.,

- P(hangover) = 4%
- P(hangover | beer) = 6%.
- P(hangover | baiju) = 85%.
:::

:::
:::

::: notes
- probability A occurs given B
- vertical bar = given
- at least 1 day for each month
- the next day after I had beer
- the next day after I had baiju
:::

---

##  Bayes's Theorem

$$
P(B \mid A) = \frac{P(A \cap B)}{P(A)}
$$

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$

$$
P(A \mid B) \times P(B) =  P(B \mid A) \times P(A)
$$


::: box
$$
P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}
$$
:::

::: fragment
### Why is this useful?
:::

---

<img src="images/fip_1.png" width="100%" style="display: block; margin: auto;"/>

---

<img src="images/fip_2.png" width="100%" style="display: block; margin: auto;"/>

---

<img src="images/fip_3.png" width="100%" style="display: block; margin: auto;"/>

---

<img src="images/fip_4.png" width="100%" style="display: block; margin: auto;"/>

---

<img src="images/fip_5.png" width="100%" style="display: block; margin: auto;"/>

---

<img src="images/fip_6.png" width="100%" style="display: block; margin: auto;"/>

---

## Forward and Inverse Problems


::: columns
::: {.column width="50%"}
::: incremental
- X: 3 red balls, 5 white balls

- Y: 1 red balls, 3 white balls

- Randomly choose a bag X or Y

- $P(A)$: [Choosing X]{.blue}

- $P(B)$: [Drawing a red ball]{.blue}

- $P(B \mid A)$: [Getting a red ball from X]{.blue}

- $P(A \cap B)$: [Choosing X and drawing a red ball]{.blue}

- $P(A \mid B)$: [Picked X, because you got a red ball(?)]{.orange}
:::
:::

::: {.column width="50%"}

::: incremental
- $P(A) = 1/2$

- $P(B \mid A) = 3/8$

- $P(B)$ = 1/2 $\times$ 3/8 + 1/2 $\times$ 1/4 = 5 /16

- $P(A \cap B)$ = 1/2 $\times$ 3/8 = 3/16

- $P(A \mid B)$ = $P(A \cap B) / P(B)$ = (3/16) / (5/16) = 3/5
:::
:::
:::

::: notes
- Best example seems to be bags and balls

- When you got a red ball, which bag did you choose?
:::

---

<img src="images/bayes_re.png" width="100%" style="display: block; margin: auto;"/>

---

$$
P(\mathrm{Parameter} \mid \mathrm{Data}) = \frac{P(\mathrm{Data} \mid \mathrm{Parameter}) \times P(\mathrm{Parameter})}{P(\mathrm{Data})}
$$

::: small
::: incremental
- $P(\mathrm{Parameter} \mid \mathrm{Data})$

  - When you got your data, what were the parameters behind the data (e.g., coefficients of regressions)?

- $P(\mathrm{Data} \mid \mathrm{Parameter})$

  - When you know your parameters, what is the probability of getting your data? (i.e., **likelihood**)

- $P(\mathrm{Parameter})$

  - Probability to get your parameters. This is what you assume for your parameter before you see any data (i.e., **prior**).

- $P(\mathrm{Data})$

  - Independent with parameters (i.e., constant)
:::
:::

---

<img src="images/bayes_re_2.png" width="100%" style="display: block; margin: auto;"/>

# Prior

```{r}
my_prior <- function(p) p^50 * (1 - p)^ 50
my_like <- function(p) p^2 * (1 - p)^1
x <- seq(0, 1, length = 100)
y <- my_prior(x)
y2 <- my_like(x)
y3 <- y * y2

p1 <- tibble(x, Prior = y * 10^29.5, Likelihood = y2, Posterior = y3 * 10^30.5) |>
  pivot_longer(Prior:Posterior) |>
  mutate(name = factor(name, levels = c("Posterior", "Likelihood", "Prior"))) |>
  ggplot(aes(x = x, y = value, color = name)) +
  geom_line(size = 1.5) +
  ylab("Probability") +
  xlab("Parameter") +
  theme(
    legend.position = c(0.15, 0.7),
    legend.text = element_text(size = 28),
    legend.title = element_blank(),
    axis.text.y = element_blank()
    )
p1

ggsave("images/priors.png", p1, width = 10, height = 6)
```

## Prior (coins)

### MLE

- A: 2 head out of 3 tosses -> 2/3 = 0.666
- B: 60 heads out of 100 tosses -> 60/100 = 0.6

::: fragment
::: columns
::: {.column width="50%"}

### Bayesian

- $L_A = {}_3C_2 p^2 (1-p)^1$

- $L_B = {}_{100}C_{60} p^{60} (1-p)^{40}$

- $\mathrm{Prior} \propto  p^{50} (1-p)^{50}$
  - Beta distribution with mean 0.5 and small variance

:::

::: {.column width="50%"}
```{r}
my_prior <- function(p) p^50 * (1 - p)^ 50
x <- seq(0, 1, length = 100)
y <- my_prior(x)

tibble(x, prior = y * 10^29.5) %>%
  ggplot(aes(x, prior)) +
  geom_line(size = 1.5) +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 28),
    axis.title = element_text(size = 32))

```

:::
:::
:::

---

## Prior (coins)

::: columns
::: {.column width=50%}
::: incremental
- $Post_A \propto p^2 (1-p)^1 \times p^{50} (1-p)^{50}$

  - $Post_A \propto p^{52} (1-p)^{51}$

  - $Post_A' = 52p^{51}(1-p)^{51}-$ \n $51p^{52}(1-p)^{50}$

  -  p = 52/103 = 0.5048

- $Post_B \propto p^{60} (1-p)^{40} \times p^{50} (1-p)^{50}$

  - $Post_B \propto p^{110} (1-p)^{90}$

  - p = 110/200 = 0.55
:::
:::
::: {.column width=50%}
::: incremental
- $Like_B$: 60 heads out of 100 tosses
- $Prior_B$: P(H) is about 50%, a reasonable assumption based on existing knowledge of coins
- $Post_B$: P(H) is likely between 50% and 60% (55% in this example)
-  We usually have some sense of a scale about parameters, we can legally use that information
:::
:::
:::

## Priors and ecology (simple linear model; *y* = *ax* + *b*)

::: incremental
- Considering variables *x* = {-3, ..., 3} and *y* = {-6, ..., 4}. At this point, we don't know if there is a correlation between *x* and *y*.
- However, given the similar scales of *x* and *y*, it's reasonable to guess that *a* falls within a narrow range (-1000 < *a* < 1000 or -5 < *a* < 5 ?).
- For example, *y* = 100 *x* + 2 doesn't work (blue line).
:::

::: fragment
```{r, fig.height=5, fig.width=5}
set.seed(123)
x1 <- rnorm(100)
y1 <- rnorm(100, 2 * x1 - 1)
p1 <- tibble(x1, y1) |>
  ggplot() +
  xlab("x") +
  ylab("y") +
  geom_point(aes(x1, y1), size = 5) +
  geom_abline(intercept = 2, slope = 1000, col = "blue", size = 2) +
  theme(
    axis.title   = element_text(size = 32),
    axis.text    = element_text(size = 28))
p1
```
:::

## Priors and ecology (simple linear model; *y* = *ax* + *b*)

- Considering variables *x* = {-3, ..., 3} and *y* = {-6, ..., 4}. At this point, we don't know if there is a correlation between *x* and *y*.
- However, given the similar scales of *x* and *y*, it's reasonable to guess that *a* falls within a narrow range (-1000 < *a* < 1000 or -5 < *a* < 5 ?).

```{r, fig.height=7, fig.width=18}
x1 <- seq(-2000, 2000, length.out = 100)
y1 <- dnorm(x1, mean = 0, sd = 1000)
p1 <- tibble(x1, y1) |>
  ggplot() +
  xlab(expression("a")) +
  ylab("Density") +
  ggtitle("N(0, 1000): Super-vague") +
  geom_line(aes(x1, y1)) +
  ylim(c(0, 0.01)) +
  theme(
    axis.title   = element_text(size = 32),
    axis.text    = element_text(size = 28))

x2 <- seq(-10, 10, length.out = 100)
y2 <- dnorm(x2, mean = 0, sd = 2.5)
p2 <- tibble(x2, y2) |>
  ggplot() +
  xlab("a") +
  ylab("Density") +
  ggtitle("N(0, 2.5): Weakly-informative") +
  geom_line(aes(x2, y2)) +
  theme(
    axis.title   = element_text(size = 32),
    axis.text    = element_text(size = 28))

p1 + p2
```


## Priors and ecology (multilevel model: group-level differences)

::: columns
::: {.column width=40%}

```{r, fig.height=8}
n_sp <- 5
x <- seq(-3, 3, length.out = 100)
b <- rnorm(n_sp, mean = 3, sd = 0.8)
a <- 2

y <- map(b, \(b)(a*x + b))
tibble(y = unlist(y), x = rep(x, n_sp), .id = rep(letters[1:n_sp], each = 100)) |>
  ggplot(aes(x = x, y = y , col = .id)) +
  geom_line(size = 1.5) +
  scale_color_viridis_d() +
  theme(legend.position = "none",
    axis.title   = element_text(size = 52),
    axis.text    = element_text(size = 48))
```

:::
::: {.column width=60%}
::: incremental
- $y_i = ax_i + b_j$
  - If the parameter $b_j$ is similar within each group (e.g., species differences, site differences):
  - Likelihood: $y_i \sim \mathcal{N}(ax_i + b_j, \sigma)$
  - Prior: $b_j \sim \mathcal{N}(\mu_b, \tau)$
:::
:::
:::

## Priors and ecology (multilevel model: autocorealtion)

::: columns
::: {.column width="30%"}
![](images/spatial.png)
:::

::: {.column width="70%"}

- If the data $y_i$ is similar to the surrounding samples (e.g., spatial autocorrelation):

::: incremental
  - Likelihood: $y_i \sim \mathcal{N}(\mu + \tilde{r_i}, \sigma)$
  - Prior: $\tilde{r_i} = r_{m, n} \sim \mathcal{N}(\phi_{m,n}, \tau)$
  - $\phi_{m,n} = (r_{m-1, n-1} + r_{m, n-1} + r_{m+1, n-1} +$ \n $r_{m-1, n} + r_{m+1, n} +$ \n  $r_{m+1, n-1} + r_{m+1, n} + r_{m+1, n+1})/8$
:::
:::
:::

# Multilevel model

## Baseball statistics

::: footer
https://www.mlb.com/stats/batting-average
:::

<img src="images/baseball.png" width =100% style="display: block; margin: auto;"/>

::: notes
- batting average estimates
:::

## Eight school problem

::: footer
[1] Rubin, D. B. Estimation in parallel randomized experiments. Journal of Educational Statistics 6, 377‚Äì401 (1981).

[2] Gelman, A. et al. Bayesian Data Analysis, Third Edition. (Chapman & Hall/CRC, 2013).
:::

![](images/eight_schools.png)

::: notes
- S A T
- data has 8 rows with mean effects and SD of the effects
- not straightforward
:::

---

## Eight species problem

::: footer
Newly developed example
:::

What is the survival rate?

::: columns
::: {.column width="30%"}
```{r}
dummy_simple <- targets::tar_read(dummy_simple)
dummy_simple |>
  dplyr::select(sp:suv) |>
  knitr::kable(format = "html") |>
  kable_material(c("striped", "hover"), full_width = FALSE)
```
:::

::: {.column width="70%"}

::: incremental

- suv $\sim$ B(n, p) (Binomial distribution)

- p  = suv / n (Maximum likelihood estimation: MLE)

:::

::: fragment
```{r, echo = TRUE}
sum(dummy_simple$suv) / sum(dummy_simple$n)
```
```{r}
suv_pool <- sum(dummy_simple$suv) / sum(dummy_simple$n)
```

:::

::: incremental

- If we pool all the data, the survival rate will be about `r suv_pool |> round(3)`

:::

:::

:::

---


## Eight species problem (separate estimates)

What is the survival rate?

::: columns
::: {.column width="35%"}
```{r}
dummy_simple <- tar_read(dummy_simple)
dummy_simple |>
  dplyr::select(sp:suv, p_like) |>
  knitr::kable(format = "html") |>
  kable_material(c("striped", "hover"), full_width = FALSE)
```
:::

::: {.column width="65%"}
::: incremental
- If we estimate each species separately, survival rates will be `p_like`
:::

:::

:::

---

## Eight species problem (separate estimates)

What is the survival rate?

::: columns
::: {.column width="40%"}
```{r}
dummy_simple <- tar_read(dummy_simple)
dummy_simple |>
  knitr::kable(format = "html") |>
  kable_material(c("striped", "hover"), full_width = FALSE)
```
:::

::: {.column width="60%"}

::: incremental
::: small
- `p_true` ranges [`r min(dummy_simple$p_true)`, `r max(dummy_simple$p_true)`]

- `p_like` ranges [`r min(dummy_simple$p_like)`, `r max(dummy_simple$p_like)`]
- The estimate shows the larger variation

- Because of the small sample size (common in ecological studies)
:::
:::

::: fragment
```{r, fig.height=3.5, fig.width=7}
ggplot(dummy_simple, aes(x = p_true, y = p_like)) +
  geom_point(size = 12, pch = 21) +
  geom_abline(slope = 1, intercept = 0, lty = 2) +
  annotate(geom = "text", x = 0.34, y = 0.38, label = "1:1", size = 10, angle = 18) +
  xlab("True survival rate") +
  ylab("Estimated \nsurvival rate")
```
:::

:::

:::

::: notes
- These data were simulated based on `p_true`, but the ML estimates are slightly different
- we want to improve the estimates
:::

::: footer
We know `p_true` because we generated this data using `p_true` as a known parameter. In practical scenarios, we don't know `p_true`.
:::


---

## Two extreme cases (pooled estimates)

- $S_i \sim \mathcal{B}(N_i, p)$

![](images/pool.png)
<!-- <img src="images/pool.png" width="60%" style="display: block; margin: auto;"/> -->

::: incremental
- This model doesn't consider the variation among species
:::

---

## Two extreme cases (separate estimates)

- $S_i \sim \mathcal{B}(N_i, p_i)$

![](images/separate.png)
<!-- <img src="images/separate.png" width="60%" style="display: block; margin: auto;"/> -->

- This model assumes that survival rates are 100% independent among species

::: notes
- if we have infinite amount of data, we can estimate survival rates separately
- usually, we don't have enough data (model tends to overfit)
- We need to find a balance between these two extremes
:::

---

## More realistic estimates (multilevel models)

- $S_i \sim \mathcal{B}(N_i, p_i)$

- $z_i \sim \mathcal{N}(\mu, \sigma)$ where $z_i = \mathrm{logit}(p_i) = \mathrm{log}\frac{p_i}{1 - p_i}$

<!-- ![](images/multilevel.png) -->
<img src="images/multilevel.png" width="80%" style="display: block; margin: auto;"/>

---

## $\sigma$ determines species variation

The overall survival rate is 0.5 in this example.
We have some sense of a scale for $\sigma$.

::: columns
::: {.column width=50%}
```{r, fig.height=10}
set.seed(123)
z <- 0
sig1 <- 0.1
sig2 <- 1
sig3 <- 3
n_sp <- 8
sig <- 0.2

xx <- seq(-3, 3, length = 100)
mu1 <- rnorm(n_sp, z, sig1)
mu2 <- rnorm(n_sp, z, sig2)
mu3 <- rnorm(n_sp, z, sig3)

data <- tibble(mu1 = as.list(mu1), mu2 = as.list(mu2), mu3 = as.list(mu3), sp = LETTERS[1:8]) |>
  mutate(y1 = map(mu1, \(mu)dnorm(xx, mu, sig))) |>
  mutate(y2 = map(mu2, \(mu)dnorm(xx, mu, sig))) |>
  mutate(y3 = map(mu3, \(mu)dnorm(xx, mu, sig))) |>
  unnest(cols = c(mu1, mu2, mu3, y1, y2, y3))  |>
  mutate(xx = rep(xx, 8))

plot_density <- function(data, y, title_suffix) {
  ggplot(data, aes(x = logistic(xx), y = {{y}}, col = sp)) +
    geom_line() +
    xlab("p") +
    ylab("Density") +
    theme(legend.position = "none") +
    ggtitle(paste("sigma =", title_suffix))
}

p1 <- plot_density(data, y1, "0.1")
p2 <- plot_density(data, y2, "1")
p3 <- plot_density(data, y3, "10")

p1 / p2 / p3
```
:::

::: {.column width=50%}
::: fragment
```{r, fig.height=10}
plot_sigma <- function(density_func, params, title) {
  y <- density_func(sigma, params[[1]], params[[2]])
  tibble(sigma, y) %>%
    ggplot(aes(x = sigma, y = y)) +
    geom_line() +
    xlab("Sigma") +
    ylab("Density") +
    ggtitle(title)
}

# Sigma values
sigma <- seq(0.01, 10, length = 100)

# Generating plots
p4 <- plot_sigma(dcauchy, list(0, 1), "sigma ~ Half-Cauchy(0, 1)")
p5 <- plot_sigma(dnorm, list(0, 1), "sigma ~ Half-N(0, 1)")
p6 <- plot_sigma(dnorm, list(0, 10), "sigma ~ Half-N(0, 10)")

p4 / p5 / p6
```
:::

::: incremental
<!-- - We have some sense of a scale for $\sigma$ -->
<!-- - `logistic(0.1)` = `r round(logistic(0.1), 3)`
- `logistic(1)` = `r round(logistic(1), 3)`
- `logistic(10) - 0.5` = `r round(logistic(10) - 0.5, 5)` -->
:::
:::
:::

::: notes
- on the logit scale
- we have some sense of a scale for sigma
- sigma = 0.3 -> +/- 7%
- sigma = 3 -> +/- 45%
- sigma = 10 -> +/- 49.99%
:::

---

## Stan code for a simple multilevel logistic model (non-verbal model){.small}

::: columns
::: {.column width="50%"}

```{stan, output.var="simple_logistic",file=here('stan/logistic.stan'), eval=FALSE, echo=TRUE}
```

:::

::: {.column width="50%"}

### Centered parameterization

- $S_i \sim \mathcal{B}_{logit}(N_i, z_i)$: likelihood

- $z_i \sim \mathcal{N}(\mu, \sigma)$ : prior

- $\sigma \sim \mathcal{N}(0, 1)$: prior

- $\mu \sim \mathcal{N}(0, 5)$: prior

###  Non-Centered parameterization

- $z_i = \mu + \sigma \cdot \tilde{z_i}$

- $\tilde{z_i} \sim \mathcal{N}(0, 1)$: prior
:::
:::

---

## Multilevel models yield better estimates

::: columns
::: {.column width="40%"}

```{r}
dummy_simple_re <- tar_read(dummy_simple_re)
dummy_simple_re |>
  mutate(p_bayes = round(p_bayes, 2)) |>
  knitr::kable(format = "html") |>
  kable_material(c("striped", "hover"), full_width = FALSE, font_size = 30)
```
:::

::: {.column width="50%"}

```{r, fig.height=6}
dummy_simple_re |>
  pivot_longer(c(p_like, p_bayes)) |>
  ggplot(aes(x = p_true, y = value, fill = name)) +
    # geom_jitter(size = 12, pch = 21, width = 0.001, alpha = 0.8) +
    geom_point(size = 12, pch = 21, alpha = 0.8) +
    annotate(geom = "text", x = 0.34, y = 0.38, label = "1:1", size = 10, angle = 18) +
    geom_abline(slope = 1, intercept = 0, lty = 2) +
    xlab("True survival rate") +
    ylab("Estimated \nsurvival rate") +
    theme(
      legend.position = c(0.2, 0.75),
      legend.text = element_text(size = 24),
      legend.title = element_blank(),
      legend.box.background = element_rect(color = "grey40", size = 1)
    )
```

::: small
::: incremental
- Closed symbols (`p_bayes`{.small}) align more closely with the 1:1 line, indicating more accurate estimates.

- This model compensates for limited data by using **prior** knowledge that species responses are somehow similar and compensates for limited data.
:::
:::

:::
:::


::: notes
- less influenced by overfitting for each species

- to understand prior, we need to understand Bayes's theorem a bit more
:::

---

## MLE vs. Bayesian estimation

::: columns
::: {.column width="50%"}

Bayesian estimation (`stan`)
```{stan, output.var="simple_logistic",file=here('stan/logistic.stan'), eval=FALSE, echo=TRUE}
```
:::
::: {.column width="50%"}

MLE (`lme4::glmer`)
```{r, echo=TRUE}
fit <- lme4::glmer(
   cbind(suv, n - suv) ~  (1 | sp),
   data = dummy_simple,
   family = binomial)
```
::: incremental
- We can do MLE for these kind of simple models.
- When a model is complicated, MLE often does not work well.
- Model flexibility for MLE is limited.
:::
:::
:::

::: notes
- You need to rely on functions, such as `lme4::glmer`.
:::

---

## MLE vs. Bayesian estimation

::: columns
::: {.column width="50%"}

Bayesian estimation (`brms`)
```{r, echo=TRUE, eval=FALSE}
fit <- brms::brm(
  suv | trials(n) ~ 1 + (1 | sp),
  data = dummy_simple,
  family = binomial())
```
:::
::: {.column width="50%"}

MLE (`lme4::glmer`)
```{r, echo=TRUE}
fit <- lme4::glmer(
   cbind(suv, n - suv) ~  (1 | sp),
   data = dummy_simple,
   family = binomial)
```
:::
:::

- Several packages, like `brms` and `rstanarm`, offer Bayesian estimation using syntax similar to `lme4`.

::: notes
- You need to rely on functions, such as `lme4::glmer`.
:::

---

## MLE vs. Bayesian estimation

::: incremental
### MLE (e.g., `lme4`{.small})

$L(\mu, \sigma) = \prod_i \int_{-\infty}^{\infty} \mathcal{B}(S_i \mid N_i, p_i) \times \mathcal{N}(\mathrm{logit}(p_i) \mid \mu, \sigma) dp_i$


- Analytically find $\mu$ and $\sigma$ to maximize $L$

- An analytical solution is often not available (this example is easy though)

### Bayesian estimation

$P(\mu, \sigma \mid S_i, N_i) \propto \prod_i \mathcal{B}(S_i \mid N_i, p_i) \times \prod_i \mathcal{N}(\mathrm{logit}(p_i) \mid \mu, \sigma) \times$ \n
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\mathcal{N}(\mu \mid 0, 5) \times \mathcal{N}(\sigma \mid 0, 2.5)$

- Numerically find $\mu$ and $\sigma$ to maximize $P$ (aka MCMC)

- MCMC works even if an analytical solution is not available

- Bayes's theorem supports the use of MCMC
:::

::: notes
posterior distribution is proportional to the likelihood times the prior
:::

## Summary

::: incremental

### Why Bayesian estimation is useful

- We can use a priori information about parameters in our model

- Models are flexible

- MCMC works even if models are complicated

### Why multilevel models are important

- Multilevel models have a good balance between pooled estimates and separate estimates, which is useful for practical sample sizes

- Multilevel models handle nested or hierarchical data structures, a common and important scenario in ecological research (e.g., trees within species, community within sites, etc.).

:::

## References

- [Gelman, A. et al. Bayesian Data Analysis, Third Edition. (Chapman & Hall/CRC, 2013)](http://www.stat.columbia.edu/~gelman/book/)

- [Stan User's Guide](https://mc-stan.org/docs/stan-users-guide/index.html)

- [Stan Functions Reference](https://mc-stan.org/docs/functions-reference/index.html)

- [AIcia Solid Project](https://youtu.be/mX_NpDD7wwg) (in Japanese and Math)

